{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53c781c",
   "metadata": {},
   "source": [
    "# Model selection & comparison\n",
    "\n",
    "_Alex Malz (LINCC@CMU)_\n",
    "_LSSTC Data Science Fellowship Program #16_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37ab94",
   "metadata": {},
   "source": [
    "This problem session is a bit of a \"choose your own adventure,\" though all will cover the Bayes factor and some information-theoretic quantities.\n",
    "You find yourself in a dark, haunted forest and reach a fork in the road. . .\n",
    "- You can play it safe and follow Adam Mantz's version of the [lecture](https://github.com/KIPAC/StatisticalMethods/blob/1a8d82d6e54c421fb22f2e891293f220bf257da1/chunks/modelevaluation.ipynb) and [problem set](https://github.com/KIPAC/StatisticalMethods/blob/1a8d82d6e54c421fb22f2e891293f220bf257da1/problems/model_evaluation.ipynb) from a few years ago. It will cover the Bayesian evidence and information criteria (like the AIC, but appropriate for Bayesian statistics) for a simple exponential model.\n",
    "- You can look at the other notebook [here](https://github.com/aimalz/dsfp2022/blob/main/metrics-toyish.ipynb). and do those same exercises from Adam's notebook above, but for data that's realistic for an astrophysical problem (Type Ia SNe for constraining cosmological models and parameters).\n",
    "- You can proceed with this notebook, which uses some real MCMC chains from a real paper, where the \"truth\" isn't known and there are tough questions of whether each model comparion metric is appropriate given the assumptions that went into the data in hand.\n",
    "\n",
    "And, just like a \"choose your own adventure\" novel, you can always try the others later on or switch if you're not having fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774035bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainconsumer import ChainConsumer\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7492099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are to make the plots from the paper, which used an older version of Python.\n",
    "import pylab as mplot\n",
    "%pylab inline\n",
    "\n",
    "mplot.rc('xtick', labelsize=16) \n",
    "mplot.rc('ytick', labelsize=16) \n",
    "\n",
    "Color = ['#404096','#57A3AD','#D92120','#DEA73A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8463ac",
   "metadata": {},
   "source": [
    "## Data & Context\n",
    "\n",
    "Let's take a close look at the procedure of [Chang+18](https://doi.org/10.1093/mnras/sty2902), in which the data from four completed weak lensing surveys, CFHTLenS, KiDS450, DES-SV, and DLS, were reanalyzed under common assumptions, i.e. priors.\n",
    "__TODO: add links to surveys__\n",
    "The analysis focuses on constraining the matter density $\\Omega_{m}$ and a measure of clustering strength, $S_{8} = \\sigma_{8}\\sqrt{\\Omega_{m} / 0.3}$.\n",
    "\n",
    "We have the MCMC chains resulting from fits to the data of each survey under their published priors and model assumptions as well as under shared priors and the same model assumptions.\n",
    "The paper originally only published [the code necessary to do the sampling](https://zenodo.org/record/1404447) rather than the MCMC samples themselves, so special thank you to [Chihway Chang](https://chihway.github.io/) for providing these __TODO: add persistent link to chains__ and the code to ingest/plot them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s8(omegam, sigma8):\n",
    "    return sigma8 * np.sqrt(omegam / 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7e564",
   "metadata": {},
   "source": [
    "Let's recreate Figure 6 from the paper.\n",
    "This shows the published constraints for two of the four surveys, DES and DLS, because the other two, CFHTLenS and KiDS, formatted their chains differently so the files are pretty big for GitHub. \n",
    "You may need to change the paths to the files once you download them from [here](https://github.com/aimalz/dsfp2022/tree/main/data).\n",
    "__TODO: update with DOI__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfhtlens_paper = 'data/official/CFHTLenS/fiducialrun'\n",
    "# kids_paper = 'data/official/KiDS450/kids450dir'\n",
    "des_paper = 'data/official/DES/dessv_chain_reduced_v2.txt'\n",
    "dls_paper = 'data/official/DLS/DLS_chain.txt'\n",
    "\n",
    "data_params = []\n",
    "weights = []\n",
    "file_list = [None,#cfhtlens_paper, \n",
    "             None,#kids_paper, \n",
    "             des_paper, dls_paper]\n",
    "probs_pub = []\n",
    "\n",
    "# Om = np.array([])\n",
    "# s8 = np.array([])\n",
    "# ww = np.array([])\n",
    "# for i in range(8):\n",
    "#     data = np.loadtxt(file_list[0]+'_'+str(i+1)+'.txt')\n",
    "#     N = len(data)\n",
    "#     Om = np.concatenate((Om, data[int(N*0.3):,9]), axis=0)\n",
    "#     s8 = np.concatenate((s8, data[int(N*0.3):,13]), axis=0)\n",
    "#     ww = np.concatenate((ww, data[int(N*0.3):,0]), axis=0)\n",
    "# data_params.append([Om, get_s8(Om, s8)])\n",
    "# weights.append(ww)\n",
    "\n",
    "# Om = np.array([])\n",
    "# s8 = np.array([])\n",
    "# ww = np.array([])\n",
    "# for i in range(8):\n",
    "#     data = np.loadtxt(file_list[1]+'_'+str(i+1)+'.txt')\n",
    "#     N = len(data)\n",
    "#     Om = np.concatenate((Om, data[int(N*0.3):,10]), axis=0)\n",
    "#     s8 = np.concatenate((s8, data[int(N*0.3):,14]), axis=0)\n",
    "#     ww = np.concatenate((ww, data[int(N*0.3):,0]), axis=0)\n",
    "# data_params.append([Om, get_s8(Om, s8)])\n",
    "# weights.append(ww)\n",
    "\n",
    "data = np.loadtxt(file_list[2])\n",
    "os.system(\"cat \"+file_list[2]+\" | tail -3 | head -1 | sed s/'='/' '/|awk '{print $2}'>nsample\")\n",
    "nsample = int(np.loadtxt('nsample'))\n",
    "weights.append(data[-nsample:,14])\n",
    "data_params.append([data[-nsample:,0], data[-nsample:,3]*(data[-nsample:,0]/0.3)**0.5]) #12\n",
    "\n",
    "data = np.loadtxt(file_list[3])\n",
    "N = len(data)\n",
    "weights.append(data[:,0])\n",
    "data_params.append([data[:,2], data[:,3]*(data[:,2]/0.3)**0.5]) \n",
    "\n",
    "data_params_published = data_params\n",
    "\n",
    "c = ChainConsumer()\n",
    "# c.add_chain(data_params[3], weights=weights[3], name='')\n",
    "c.add_chain(data_params[0], weights=weights[0], name='', parameters=[r\"$\\Omega_{\\rm m}$\", r\"$S_{8}$\"])\n",
    "# c.add_chain(data_params[2], weights=weights[2], name='')\n",
    "c.add_chain(data_params[1], weights=weights[1], name='')\n",
    "\n",
    "c.configure(colors=Color, label_font_size=18, contour_label_font_size=20, \n",
    "            tick_font_size=20, linewidths=[1.5,1.5,1.5,1.5], sigma2d=False, shade=True, \n",
    "            kde=1.5, shade_alpha=[0.2,1,0.2,0.7], bar_shade=True, sigmas=[0,1,2])\n",
    "fig = c.plotter.plot(extents=[(0.02,0.95),(0.4,1.1)])\n",
    "mplot.text(-27,0.45,'Published Baseline', color='k', fontsize=20)\n",
    "fig.set_size_inches(4.5 + fig.get_size_inches())\n",
    "\n",
    "mplot.text(1,1.30,'DLS', color=Color[0], fontsize=20)\n",
    "# mplot.text(1,1.25,'CFHTLenS', color=Color[0], fontsize=20)\n",
    "mplot.text(1,1.20,'DES-SV', color=Color[1], fontsize=20)\n",
    "# mplot.text(1,1.15,'KiDS-450', color=Color[0], fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebf93f3",
   "metadata": {},
   "source": [
    "### Problem 0a\n",
    "\n",
    "\"$\\chi$ by eye\" is an irresistably intuitive qualitative model comparison metric.\n",
    "Based on the above figure, which survey looks like it provides the best constraints?\n",
    "Does your answer differ if you consider $\\Omega_{m}$ and $S_{8}$ individually vs. jointly on both parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b3672a",
   "metadata": {},
   "source": [
    "This block of code ingests the chains for the common assumptions case, meaning the same parameters were held constant vs. fit in the MCMC sampling, and the same priors were used, and then plots the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfhtlens2_g = 'data/matched/CFHTLenS/mcmc_cfhtlens_matched2_covg.txt'\n",
    "kids2_g = 'data/matched/KiDS450/mcmc_kids_matched2_covg.txt'\n",
    "des2_g = 'data/matched/DES/mcmc_des_matched2_covg.txt'\n",
    "dls2_g = 'data/matched/DLS/mcmc_dls_matched2_covg.txt'\n",
    "\n",
    "data_params = []\n",
    "weights = []\n",
    "file_list = [cfhtlens2_g, \n",
    "             kids2_g, \n",
    "             des2_g, dls2_g]\n",
    "\n",
    "data = np.loadtxt(file_list[0])\n",
    "os.system(\"cat \"+file_list[0]+\" | tail -3 | head -1 | sed s/'='/' '/|awk '{print $2}'>nsample\")\n",
    "nsample = int(np.loadtxt('nsample'))\n",
    "weights.append(data[-nsample:,-1])\n",
    "data_params.append([data[-nsample:,21], data[-nsample:,20]*(data[-nsample:,21]/0.3)**0.5]) #12\n",
    "    \n",
    "data = np.loadtxt(file_list[1])\n",
    "os.system(\"cat \"+file_list[1]+\" | tail -3 | head -1 | sed s/'='/' '/|awk '{print $2}'>nsample\")\n",
    "nsample = int(np.loadtxt('nsample'))\n",
    "weights.append(data[-nsample:,-1])\n",
    "data_params.append([data[-nsample:,15], data[-nsample:,14]*(data[-nsample:,15]/0.3)**0.5]) #12\n",
    "\n",
    "data = np.loadtxt(file_list[2])\n",
    "os.system(\"cat \"+file_list[2]+\" | tail -3 | head -1 | sed s/'='/' '/|awk '{print $2}'>nsample\")\n",
    "nsample = int(np.loadtxt('nsample'))\n",
    "weights.append(data[-nsample:,-1])\n",
    "data_params.append([data[-nsample:,13], data[-nsample:,12]*(data[-nsample:,13]/0.3)**0.5]) #12\n",
    "\n",
    "data = np.loadtxt(file_list[3])\n",
    "os.system(\"cat \"+file_list[3]+\" | tail -3 | head -1 | sed s/'='/' '/|awk '{print $2}'>nsample\")\n",
    "nsample = int(np.loadtxt('nsample'))\n",
    "weights.append(data[-nsample:,-1])\n",
    "data_params.append([data[-nsample:,9], data[-nsample:,8]*(data[-nsample:,9]/0.3)**0.5]) #12\n",
    "\n",
    "data_params_matched = data_params\n",
    "\n",
    "c = ChainConsumer()\n",
    "c.add_chain(data_params[3], weights=weights[3], parameters=[r\"$\\Omega_{\\rm m}$\", r\"$S_{8}$\"], name='')\n",
    "c.add_chain(data_params[0], weights=weights[0], name='')\n",
    "c.add_chain(data_params[2], weights=weights[2], name='')\n",
    "c.add_chain(data_params[1], weights=weights[1], name='')\n",
    "\n",
    "c.configure(colors=Color, label_font_size=18, contour_label_font_size=20, \n",
    "            tick_font_size=20, linewidths=[1.5,1.5,1.5,1.5], sigma2d=False, shade=True, \n",
    "            kde=1.5, shade_alpha=[0.2,1,0.2,0.7], bar_shade=True, sigmas=[0,1,2])\n",
    "\n",
    "fig = c.plotter.plot(extents=[(0.02,0.95),(0.4,1.1)])\n",
    "\n",
    "mplot.text(1,1.30,'DLS', color=Color[0], fontsize=20)\n",
    "mplot.text(1,1.25,'CFHTLenS', color=Color[2], fontsize=20)\n",
    "mplot.text(1,1.20,'DES-SV', color=Color[1], fontsize=20)\n",
    "mplot.text(1,1.15,'KiDS-450', color=Color[3], fontsize=20)\n",
    "\n",
    "mplot.text(-12,0.45,'Matched (i)', color='k', fontsize=20)\n",
    "\n",
    "fig.set_size_inches(4.5 + fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c2f75",
   "metadata": {},
   "source": [
    "### Problem 0b\n",
    "\n",
    "Based on the above figure, which survey looks like it provides the best constraints?\n",
    "Does your answer differ if you consider $\\Omega_{m}$ and $S_{8}$ individually vs. jointly on both parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c4d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb07b501",
   "metadata": {},
   "source": [
    "### Problem 0c\n",
    "\n",
    "Now, Frankenstein the code that made the two plots above to make separate plots for each survey showing the contours for both sets of analysis assumptions on the same axes.\n",
    "Which set of assumptions appears more favorable for each survey, the published or the matched conditions?\n",
    "Does your answer differ if you consider $\\Omega_{m}$ and $S_{8}$ individually vs. jointly on both parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9dc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05996fb6",
   "metadata": {},
   "source": [
    "## Bayesian model comparison\n",
    "\n",
    "The Bayes Factor $BF_{0,1} = \\frac{P(x | \\Theta_{0})}{P(x | \\Theta_{1}} = \\frac{\\frac{P(\\Theta_{0} | x)}{P(\\Theta_{1} | x)}}{\\frac{P(\\Theta_{0})}{P(\\Theta_{1})}}$ compares posteriors estimated from the same data $x$ under different models $\\Theta_{i}$ with different priors $P(\\Theta_{i})$.\n",
    "We'll try doing this over discrete chunks of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8721902",
   "metadata": {},
   "source": [
    "The first line of each of the files included here contains a line saying what the columns are.\n",
    "For the matched files and the published versions for DES and DLS, it looks like this:\n",
    "`#cosmological_parameters--omch2\tcosmological_parameters--ombh2\tcosmological_parameters--log1e10as\tcosmological_parameters--n_s\tcosmological_parameters--h0\twl_photoz_errors--bias_1\twl_photoz_errors--bias_2\twl_photoz_errors--bias_3\tshear_calibration_parameters--m1\tshear_calibration_parameters--m2\tshear_calibration_parameters--m3\tintrinsic_alignment_parameters--a\tCOSMOLOGICAL_PARAMETERS--SIGMA_8\tCOSMOLOGICAL_PARAMETERS--OMEGA_M\tlike\tpost\tweight`\n",
    "(The published results for KiDS and CFHTLenS provide a separate file with the parameter names and ordering.)\n",
    "_tl;dr the likelihoods and posteriors of each set of sampled parameters are the second- and third-to-last columns._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7729d06",
   "metadata": {},
   "source": [
    "### ~~Problem 0~~\n",
    "\n",
    "In this context, the \"data\" $x$ is a vector the jointly sampled parameters, including at least two cosmological parameters, $S_{8}$, and $\\Omega_{m}$, and a bunch of nuisance parameters for the systematic error model.\n",
    "Each value thereof is associated with a likelihood and a posterior in the files containing the samples.\n",
    "\n",
    "Write a function to isolate the likelihoods and posteriors from the files containing the chains (using `chainconsumer`, `numpy`, `pandas`, or whatever else you prefer).\n",
    "__TODO: provide this function already__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a4e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6da9e95f",
   "metadata": {},
   "source": [
    "### ~~Problem 0~~\n",
    "\n",
    "Write a function to turn samples into evaluations of the posterior probability density on a common grid in order to evaluate metrics.\n",
    "Hint: Chunk up the values of the parameters (say, on a grid) and then average the included likelihoods and posteriors within the chunk to approximate the continuous probabilities, or use a canned histogramming function from, e.g. `numpy` or `scipy`.\n",
    "__TODO: provide this function already__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f90db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a03301",
   "metadata": {},
   "source": [
    "### Problem 1a\n",
    "\n",
    "Calculate the one-dimensional Bayes factor for each survey just for the parameter $S_{8}$, comparing the different analysis models.\n",
    "Which model is better for each data set?\n",
    "Do the same for just $\\Omega_{m}$.\n",
    "Does your answer differ depending on which of $\\Omega_{m}$ and $S_{8}$ you're looking at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f245d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e1194a",
   "metadata": {},
   "source": [
    "### Problem 1b\n",
    "\n",
    "Calculate the two-dimensional Bayes factor for each survey for $\\Omega_{m}$ and $S_{8}$ jointly under the different analysis models, one data set at a time.\n",
    "Does your assessment of which model is better change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83be7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f685f6ca",
   "metadata": {},
   "source": [
    "### Problem 1c\n",
    "\n",
    "Compare the matched-assumption constraints across all the data sets and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b14252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37b117f3",
   "metadata": {},
   "source": [
    "### Problem 1d\n",
    "\n",
    "In the previous part, there are some nuances about whether the Bayes Factor is even applicable.\n",
    "What's working in its favor, and what assumptions are violated?\n",
    "How is the interpretation affected?\n",
    "_Highlight the cell below for answer._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f75f8",
   "metadata": {},
   "source": [
    "<font color='white'>\n",
    "Answer: \n",
    "The assumptions are the same, so the term for the priors drops out, meaning the evidence ratio would be the posterior ratio (which is good because that's what we have in hand).\n",
    "However, we were also comparing different data, so p(x | Theta) doesn't have the same thing on the left hand side of the conditional, meaning it wasn't entirely kosher to calculate it across surveys.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dafcd4",
   "metadata": {},
   "source": [
    "### Problem 1e\n",
    "\n",
    "Was the Bayes Factor consistent with your \"$\\chi$ by eye\" impression?\n",
    "If not, think about what your eye versus the quantitative metric might have been picking up on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757158f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b6725bc",
   "metadata": {},
   "source": [
    "## Information-based metrics\n",
    "\n",
    "It's not strictly Bayesian, but information theory suggests additionmal metrics that tell us how well different models constrain parameters.\n",
    "What could we have done instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cceb85",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Consider the entropy $H(X) = -\\int p(x) \\log[p(x)] dx$, a cousin of the Fisher information covered in today's alternative notebook.\n",
    "Calculate and interpret the entropies of the MCMC samples under each combination of data and analysis model.\n",
    "_Hint: You can use the [`scipy.stats.entropy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html) implementation if you'd like, but don't forget to account for the conversion between discrete and continuous variables!_\n",
    "Which combination contains the most information about the cosmological parameters?\n",
    "Can you think of reasons why that might be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541ca60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5052f912",
   "metadata": {},
   "source": [
    "__TODO: more parts?__\n",
    "\n",
    "Could do the information criteria for CFHTLenS/KiDS450 under different paramters fit in their fiducial vs. matched but left it out of data due to complexity. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSFP (Python 3)",
   "language": "python",
   "name": "dsfp_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
